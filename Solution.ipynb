{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uoMdguqaIDl0"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import random\n",
        "import collections\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKXxoy-WIOMB",
        "outputId": "1116db44-1a41-4b40-d940-47e5e52c43f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is a function that helps to reduce the size of a large dataset containing images and labels. Let me explain it step by step:\n",
        "\n",
        "1. The function is called `reduce_dataset`, and it takes two main inputs:\n",
        "   - `dataset`: This is the original dataset in the form of a tuple, containing images and their corresponding labels. The images are pictures of handwritten numbers (0 to 9), and each image has a label that tells us which number it represents.\n",
        "   - `target_size`: This is the desired size we want for each label in the dataset. For example, if `target_size` is set to 600, we want to have 600 images for each number in our final reduced dataset.\n",
        "\n",
        "2. The function starts by extracting the images and labels from the input `dataset` tuple.\n",
        "\n",
        "3. It initializes two empty lists called `reduced_images` and `reduced_labels`. These lists will be used to store the smaller subset of images and labels we select.\n",
        "\n",
        "4. Next, it creates a counter object called `label_counter`, which helps count the occurrences of each label in the original dataset. This is useful to ensure we have an equal representation of each number in the reduced dataset.\n",
        "\n",
        "5. The function then goes through each unique label in the dataset. For example, it starts with label 0, then label 1, and so on up to label 9.\n",
        "\n",
        "6. For each label, it randomly selects `target_size` number of indices (positions) from the original dataset where the label matches the current label being processed. These randomly chosen indices represent the images we want to keep for that specific label.\n",
        "\n",
        "7. It then iterates through the selected indices and adds the corresponding images and labels to the `reduced_images` and `reduced_labels` lists, respectively.\n",
        "\n",
        "8. Once it has processed all the labels, the function creates a new dataset called `reduced_dataset` by converting the lists `reduced_images` and `reduced_labels` into numpy arrays.\n",
        "\n",
        "9. Finally, it returns the `reduced_dataset`, which now contains a smaller and more manageable subset of images and labels, ensuring that we have exactly `target_size` number of images for each label.\n",
        "\n",
        "The last part of the code applies this function to the original dataset `dataset`, with `target_size` set to 600 for the training dataset and 100 for the testing dataset. It then prints the number of images in the new training and testing datasets to verify that the reduction process was successful."
      ],
      "metadata": {
        "id": "hHKgbhh7IPlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def reduce_dataset(dataset, target_size):\n",
        "    \"\"\"\n",
        "    Reduce the size of the dataset to the target size for each label.\n",
        "\n",
        "    Inputs:\n",
        "        dataset (tuple): The original dataset in tuple form, containing images and labels.\n",
        "        target_size (int): The target size for each label.\n",
        "\n",
        "    Output:\n",
        "        tuple: The reduced dataset in the same format as the original dataset.\n",
        "    \"\"\"\n",
        "    images, labels = dataset\n",
        "    reduced_images = []\n",
        "    reduced_labels = []\n",
        "    label_counter = collections.Counter(labels)\n",
        "\n",
        "    for label in label_counter.keys():\n",
        "        # Select target_size random indices for each label\n",
        "        indices = random.sample([i for i, l in enumerate(labels) if l == label], target_size)\n",
        "        for index in indices:\n",
        "            reduced_images.append(images[index])\n",
        "            reduced_labels.append(label)\n",
        "\n",
        "    reduced_dataset = (np.array(reduced_images), np.array(reduced_labels))\n",
        "    return reduced_dataset\n",
        "\n",
        "# Reduce the size of the training and testing datasets\n",
        "new_training_dataset = reduce_dataset(dataset[0], target_size=600)\n",
        "new_testing_dataset = reduce_dataset(dataset[1], target_size=100)\n",
        "\n",
        "# Combine the reduced training and testing datasets into a new dataset\n",
        "new_dataset = (new_training_dataset, new_testing_dataset)\n",
        "\n",
        "# Verify the sizes of the new dataset\n",
        "print(\"Number of images in the new training dataset:\", len(new_dataset[0][0]))\n",
        "print(\"Number of images in the new testing dataset:\", len(new_dataset[1][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJC7ZEsIITYV",
        "outputId": "f509d271-5ddf-43c9-abf2-7a5cc92aaeb6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images in the new training dataset: 6000\n",
            "Number of images in the new testing dataset: 1000\n"
          ]
        }
      ]
    }
  ]
}